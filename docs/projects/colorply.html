<!--
File: colorply.html
Creation: Sunday July 5th 2020
Author: Arthur Dujardin
--------
Copyright (c) 2020 Arthur Dujardin
-->

<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <meta http-equiv="X-UA-Compatible" content="ie=edge" />
        <title>Arthur D.</title>

        <!-- CSS Layouts -->
        <link rel="stylesheet" href="../static/css/root.css" />
        <link rel="stylesheet" href="../static/css/style.css" />
        <link rel="stylesheet" href="../static/css/layout.css" />
        <link rel="stylesheet" href="../static/css/fonts.css" />
        <link rel="stylesheet" href="../static/css/colors.css" />
        <link rel="stylesheet" href="../static/css/project.css" />
        <link rel="stylesheet" href="../static/css/navbar.css" />
        <link rel="stylesheet" href="../static/css/canvas.css" />

        <!-- JQuery -->
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
        <!-- Font Awesome -->
        <script src="https://use.fontawesome.com/releases/v5.5.0/js/all.js"></script>
        <!-- TweenMax -->
        <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/1.20.4/TweenMax.min.js"></script>
        <!-- Scrol Reveal -->
        <script src="https://unpkg.com/scrollreveal"></script>
        <!-- Latex on HTML -->
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <!-- Highlight Script -->
        <link rel="stylesheet" href="../dist/css/atom-one-light.css" />
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.2.0/highlight.min.js"></script>
        <script>
            hljs.initHighlightingOnLoad();
        </script>
        <!-- Chart -->
        <script src="https://cdn.jsdelivr.net/npm/chart.js@2.9.3/dist/Chart.min.js"></script>
        <!-- Chart on Scroll -->
        <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-deferred@1"></script>
    </head>

    <body>
        <!-- Main NavBar -->
        <header id="navbar" class="padding bg-white">
            <!-- Logo -->
            <ul>
                <li><a href="../index.html" class="dark-grey">Arthur D.</a></li>
            </ul>
            <!-- Hamburger icon -->
            <div id="nav-toggle">
                <div class="nav-toggle-bar1 bg-dark-grey"></div>
                <div class="nav-toggle-bar2 bg-dark-grey"></div>
                <div class="nav-toggle-bar3 bg-dark-grey"></div>
            </div>
            <!-- Menu -->
            <nav id="nav-menu">
                <ul>
                    <li>
                        <a href="../index.html#portfolio" class="dark-grey">Work</a>
                    </li>
                    <li><a href="../about.html" class="dark-grey">About</a></li>
                    <li>
                        <a href="../contact.html" class="dark-grey">Contact</a>
                    </li>
                </ul>
            </nav>
            <!-- End Navbar Menu -->
        </header>
        <!-- End Main NavBar -->

        <!-- Home section -->
        <div id="home" class="project margin">
            <!-- Landing Page -->
            <div class="landing padding-xl">
                <!-- Description -->
                <h1 class="anim-intro">Multispectral Photogrammetry</h1>
                <h2 class="anim-intro">Internship Project</h2>

                <!-- Keywords -->
                <p class="resume light-grey keywords anim-intro">#photogrammetry #multispectral #python #pointcloud #ply</p>
                <p class="anim-intro resume">
                    Colorply is an open source
                    <span class="color-red">python</span> application which add new wavelength channels to a 3D cloud of points from a set of referenced images and uses images calibration from
                    <a href="https://micmac.ensg.eu/index.php/Accueil" target="_blank"><span class="color-red">MicMac</span></a
                    >.
                </p>
                <p class="anim-intro resume">
                    The main project was divided in two parts. First, we
                    <span class="color-red">analyzed</span> and evaluated the generated images and camera, then we <span class="color-red">classified</span> alpine vegetation from aerial multispectral
                    images - with random forest algorithm.
                </p>
                <p class="anim-intro resume">
                    This project was made at the
                    <span class="color-red">IGN</span> center.
                </p>

                <!-- Information -->
                <p class="anim-intro">
                    <span class="when">When : <span class="light-grey">07.2019</span></span>
                    <br />
                    <span class="where">Where : <span class="light-grey">Forcalquier, France</span></span>
                    <br />
                    <span class="duration">Duration : <span class="light-grey">3 days</span></span>
                    <br />
                    <span class="team">Team : <span class="light-grey">Teamwork of 5</span></span>
                </p>

                <!-- Tools -->
                <div class="tools">
                    <a class="logo num1 anim-intro color-red" href="https://github.com/arthurdjn/colorply" target="_blank">
                        <i class="fab fa-github"></i>
                    </a>
                    <a class="logo num2 anim-intro color-red" href="https://github.com/arthurdjn/colorply/tree/master/colorply" target="_blank">
                        <i class="fab fa-python"></i>
                    </a>
                </div>

                <!-- Scroll Down -->
                <div class="scroll">
                    <a href="#context" class="arrow down dark-grey anim-intro"></a>
                </div>
            </div>
            <!-- End Landing Page -->
        </div>
        <!-- End Home section -->

        <!-- Project Work section -->
        <div id="work">
            <!-- Context section -->
            <!-- Banner -->
            <div id="context" class="banner-start bg-red">
                <div class="text margin">
                    <h2 class="white margin-title">Context</h2>
                    <p class="white margin-xl">MicMac is a popular software for photogrammetry, used by researchers and made at the IGN x ENSG. However, this tool has some limitations.</p>
                    <p class="white margin-xl">
                        Because MicMac works only for RGB (or maximum 3-channels images), Colorply was created and handles as many channels as you want and complete an existing 3D cloud of points from
                        multispectral images.
                    </p>
                </div>
            </div>
            <!-- End Banner -->

            <!-- Sticky Navbar -->
            <nav id="side-menu" class="transition sticky reveal">
                <!-- change-opacity null-opacity-->
                <div class="line transition bg-light-grey"></div>
                <ul>
                    <li>
                        <a href="#context" class="transition dark-grey">Context</a>
                    </li>
                    <li>
                        <a href="#camera" class="transition dark-grey">Camera</a>
                    </li>
                    <li>
                        <a href="#photogrammetry" class="transition dark-grey">Photogrammetry</a>
                    </li>
                    <li>
                        <a href="#classification" class="transition dark-grey">Classification</a>
                    </li>
                    <li>
                        <a href="#outcomes" class="transition dark-grey">Outcomes</a>
                    </li>
                </ul>
            </nav>
            <!-- End Navbar -->

            <!-- Scroll up -->
            <div class="scroll-up sticky">
                <a href="#" class="arrow up light-grey change-color transition"></a>
            </div>
            <!-- End scroll up -->

            <!-- Camera Section -->
            <!-- Columns: big image to the right -->
            <div id="camera" class="margin reveal">
                <div class="margin-md">
                    <div class="row">
                        <div class="column expand">
                            <h1>A multispectral<br />camera</h1>
                            <p class="margin-left">
                                As this project focused on multispectral photogrammetry, we used the
                                <span class="color-red">Parrot Sequoia</span> to capture images in different wave-length. This camera is composed by four sensors RED, GREEN, NEAR INFRARED, RED EDGE,
                                RGB - from left to right.
                                <br />
                                Note that the RGB sensor does not have the same resolution has the other. It is mostly used for traditional survey and geo-referencing, and was not used for
                                multispectral photogrammetry.
                            </p>
                        </div>
                        <div class="column padding-sm expand-sm">
                            <img src="../medias/projects/colorply/parrot_sequoia.png" />
                        </div>
                    </div>
                </div>
                <p class="margin-xl">Before exploiting our camera, we calibrated each sensors and evaluated their distorison.</p>
            </div>
            <!-- End Columns -->

            <div class="margin reveal">
                <h2 class="margin-title">Drone</h2>
                <div class="margin-xl">
                    <h3>Set-up</h3>
                    <p>
                        The camera was attached to a civil drone while we flew over high altitude forest in high provence. We used a 3D printed base to fix the camera under the drone, and strap it all
                        together.
                    </p>
                    <h3>Flight</h3>
                    <p>
                        We used
                        <a href="flightio.html"><span class="color-red">Flightio</span></a>
                        to set-up our flight and ensure that each pictures overlap at least 60% - a photogrammetry constraint.
                    </p>
                    <img src="../medias/projects/colorply/drone_sequoia.png" alt="" />
                </div>

                <h2 class="margin-title">Calibration</h2>
                <div class="margin-xl">
                    <h3>Attachement of the sensors</h3>
                    <p>
                        This step is essential for the rest of our project, it allows the attachment of our 5 sensors, i.e. find the attachment vectors and the rotation matrices between the 5 Sequoia
                        sensors - in meters.
                        <br />
                        We took several photos on all channels, and used MicMac to orientate the images in 3D - we used geo-referenced targets to reference the images. Then, we were able to see and
                        measure the different attachment vectors and rotation matrices from one image to another.
                    </p>

                    <table>
                        <tr>
                            <th>Sensor</th>
                            <th>Distance (mm)</th>
                        </tr>
                        <tr>
                            <td>GRE - RED</td>
                            <td>14.0</td>
                        </tr>
                        <tr>
                            <td>NIR- REG</td>
                            <td>14.9</td>
                        </tr>
                        <tr>
                            <td>NIR - RED</td>
                            <td>15.0</td>
                        </tr>
                        <tr>
                            <td>GRE - REG</td>
                            <td>14.4</td>
                        </tr>
                        <tr>
                            <td>Diag GRE - NIR</td>
                            <td>18.7</td>
                        </tr>
                        <tr>
                            <td>Diag RED - REG</td>
                            <td>22.2</td>
                        </tr>
                    </table>

                    <p>
                        The rotation matrices were really close to identity, meaning that all sensors are pointing in the same direction. For a better accuracy in our further work, we used these
                        results as a fix base, i.e. we re-defined the distances and rotations between sensors with the ones above.
                    </p>

                    <h3>Wavelength influence</h3>
                    <p>The sequoia camera has 4 channels sensitive to different wavelengths:</p>

                    <table>
                        <tr>
                            <th>Sensor</th>
                            <th>Wavelength (nm)</th>
                        </tr>
                        <tr>
                            <td>GREEN</td>
                            <td>550</td>
                        </tr>
                        <tr>
                            <td>RED</td>
                            <td>660</td>
                        </tr>
                        <tr>
                            <td>REG</td>
                            <td>735</td>
                        </tr>
                        <tr>
                            <td>NIR</td>
                            <td>790</td>
                        </tr>
                    </table>

                    <p>Let's have a reminder of the wavelength reflectance of soil and vegetation:</p>
                    <img class="padding-sm" src="../medias/projects/colorply/wavelength.png" alt="" />
                    <p>
                        We can clearly see that in visible wavelength, soil and vegetation have similar reflectance. But the REG and NIR windows present a clear separation between these two features.
                    </p>
                    <p>
                        The Sequoia sensors do not receive the same light information from an object, an object does not reflect uniformly across all wavelengths. For example a tree returns more in
                        the green than in the red. The goal here was to quantify and understand the impact of different image composition before diving into more complex photogrammetry application.
                    </p>
                    <p>
                        We evaluated each sensors regarding their photogrammetric rendering on four different scenes: rocks, forest, urban, mix. As each scenes have different composition (vegetation,
                        roofs, rocks...), the sensors will not register the same information.
                    </p>

                    <canvas id="wavelength" width="100%" height="60vh"></canvas>
                    <button id="btn1">Points generated</button>
                    <button id="btn2">False points</button>
                    <button id="btn3">Errors</button>
                </div>
            </div>
            <!-- End Camera Section -->

            <!-- Photogrammetry Section -->
            <div id="photogrammetry" class="margin reveal">
                <h1 class="margin-md">Multispectral photogrammetry</h1>
                <h2 class="margin-title">Epipolar geometry</h2>

                <div class="margin-xl">
                    <!-- Overview -->
                    <h3>Introduction</h3>
                    <p>
                        <span class="color-red">Epipolar geometry</span> is the geometry of stereo vision. Two cameras looking toward the same scene in different positions can infer their relative
                        position and orientation and thus, construct a new scene from 2D to 3D.
                    </p>

                    <img src="../medias/projects/colorply/epipolar_geometry.png" />
                    <!-- End Overview -->

                    <!-- Start Apero -->
                    <h3>MicMac</h3>
                    <p>MicMac is powerfull tool when it comes to photogrammetry computation. It offers a great freedom on image manipulation, usefull for research.</p>
                    <p>
                        We sampled an area of forest and mix vegetation by drone, and generated cloud of points - with MicMac and the attachment vectors previously determined. As we used four sensors
                        (GRE, RED, REG, NIR), we generated four clouds of points.
                    </p>
                    <img src="../medias/projects/colorply/cloud_points_all_wavelength.gif" />
                    <h4>Mix vegetation, area located in Provence, France.</h4>
                    <p>
                        Our main objective was to merge these four models into one with four (or more) wavelength attributes. Note that even though these models share a global structure all points
                        have a unique position, so we couldn't naively 'merge' the points together. We wanted a photogrammetric solution.
                    </p>
                </div>
                <!-- End Apero -->

                <!-- Start Image Formula -->
                <h2 class="margin-title">Image Formula</h2>
                <div class="margin-xl">
                    <h3>Definition</h3>
                    <p>
                        The image formula is used to switch from image to ground coordinates system. For a point \(m\) in the image, its representation \(M\) in the ground system is linked to the
                        orientation \(R\) of the image, its optical center \(S\) and focal length \(F\). $$ m = F - \frac{k^TFR(M - S)}{k^TR(M - S)} $$
                    </p>

                    <!-- Code Block -->
                    <pre>
<code class="python">import  numpy as np


def image_formula(F, M, R, S):
    """Computes the image formula for the point M without distorsion.

    Parameters
    ----------
    F : numpy.ndarray
        Position of the autocollimation point in the image coordinate system.
    M : numpy.ndarray
        Position of the point in ground space coordinates.
    R : numpy.ndarray
        Rotation matrix representing the orientation of the image coordinate system
        in the ground space coordinate system.
    S : numpy.ndarray
        Position of the autocollimation point in the ground space coordinate system.

    Returns
    -------
    numpy.ndarray
        Image coordinates of M projected.

    """
    kT = np.array([0, 0, 1])
    R_inv = np.linalg.inv(R)
    top = kT @ F @ R_inv @ (M - S)
    bottom = kT @ R_inv @ (M - S)

    return F - top/bottom
</code>
</pre>
                    <!-- End Code Block -->

                    <h3>Distorsion correction</h3>
                    <p>
                        As the different photos were captured by a camera sensor, we corrected the distorsion artefacts \(dr\) by applying a radial model: $$ dr = ar^3 + br^5 + cr^7 $$ were \(r\) is
                        te distance \(d(PPS, m)\),
                        <br />
                        \(a, b, c\) are the distorsion coefficients - computed with a MSE algorithm.
                    </p>

                    <!-- Code Block -->
                    <pre>
<code class="python">def radial_std(m_image, pps, a, b, c):
    r"""Corrects the postion of the point according 
    to the standard radial distorsion model.

    .. note::
        We use Horner's method to evaluate :math:`ar^2 + br^4 + cr^6`
    
    Parameters
    ----------
    m_image : numpy.ndarray
        Position of the projected point in pixel.
    pps : numpy.ndarray
        Position of the point of 0 distorsion in the radial model.
    a : float
        3rd order coefficient of the distorsion polynomial.
    b : float
        5th order coefficient of the distorsion polynomial.
    c : float
        7th order coefficient of the distorsion polynomial.

    Returns
    -------
    numpy.ndarray
        Corrected point position.

    """
    r = np.linalg.norm(m_image - pps)
    rsquared= r * r

    # Horner's method
    poly = c
    poly = poly * rsquared + b
    poly = poly * rsquared + a
    poly = poly * rsquared
    
    # Correction vector
    dr = poly * (m_image - pps) 
    
    return m_image + dr
</code>
</pre>
                    <!-- End Code Block -->
                    <p>
                        This can also be done in one function:
                    </p>

                    <!-- Code Block -->
<pre>
<code class="python">def image_formula_corrected(F, M, R, S, pps, a, b, c):
    """Compute the image formula for the point M
    with distorsion.
    
    Parameters
    ----------
    F : numpy.ndarray
        Position of the autocollimation point in the image coordinate system.
    M : numpy.ndarray
        Position of the point in real space coordinates.
    R : numpy.ndarray
        Rotation matrix representing the orientation of the image coordinate system in the real space coordinate system.
    S : numpy.ndarray
        Position of the autocollimation point in the real space coordinate system.
    pps : numpy.ndarray
        Position of the point of 0 distorsion ine the radial_std model.
    a : float
        3rd order coefficient of the distorsion polynomial.
    b : float
        5th order coefficient of the distorsion polynomial.
    c : float
        7th order coefficient of the distorsion polynomial.
    
    Returns
    -------
    numpy.ndarray
        Corrected point position.
    """
    return radial_std(image_formula(F, M, R, S), pps, a, b, c)

</code>
</pre>
                    <!-- End Code Block -->

                    <h3>Multispectral cloud of points</h3>
                    <p>
                        To generate our desired model, we first generated a single cloud of points using the REG channel - best band for photogrammetry renderings. Then, we projected every points with
                        the image formula into the three remaining set of pictures i.e. GREEN, RED, NIR. The corresponding radiometry was then stored into the respective 3D point - that's where the
                        model became multispectral.
                    </p>
                    <img src="../medias/projects/colorply/image_formula.gif" alt="" />

                    <!-- Code Block -->
                    <pre>
<code class="python">def radiometry_projection(M, images_loaded, calibration):
    """This function adds a new channel to a point M, computed from the loaded images.
    Therefore, the images should be calibrated in the same reference of the point M.
    Usually, the 3D point M is part of a cloud points.

    Parameters
    ----------
    M : numpy.ndarray
        Position of the point in real space coordinates.
    images_loaded : list of Images
        List of the image loaded. 
        These images need to be referenced in the same system as the point M.
        Usually with MicMac calibrate all the images together.
    calibration : dict
        Dictionary containing the camera calibration global parameters.

    Returns
    -------
    float
        Value of the new channel.

    """
    # Load once the camera calibration
    size = calibration['size']
    F = calibration['F']
    pps = calibration['PPS']
    a = calibration['cdist']['a']
    b = calibration['cdist']['b']
    c = calibration['cdist']['c']

    n = len(images_loaded)
    L = []
    # Run all images on the scene
    for image in images_loaded:
        # Get the orientation and data from the image
        data = image.data
        R = image.R
        S = image.S
        # Projection
        m = image_formula_corrected(F, M, R, S, pps, a, b, c)
        mx = int(np.round(m[0]))
        my = int(np.round(m[1]))

        # Test if the projected point is visible from the images
        if (0 < mx < size[0]) and (0 < my < size[1]):
            L.append(int(data[my, mx]))

    return np.mean(L)
</code>
</pre>
                    <!-- End Code Block -->
                </div>
                <!-- End Image Formula -->
            </div>
            <!-- End Photogrammetry Section -->

            <!-- Classification Section -->
            <div id="classification" class="margin reveal">
                <h1 class="margin-md">Classification</h1>
                <h2 class="margin-title">Random Forest</h2>
                <div class="margin-xl">
                    <p>After generating our 4-bands 3D models, we classified four types of vegetation: terrain, grass, shrub, oak. We scored a global accuracy of 92.07% on the previous area.</p>

                    <table>
                        <tr>
                            <th></th>
                            <th>Terrain</th>
                            <th>Oak</th>
                            <th>Shrub</th>
                            <th>Grass</th>
                        </tr>
                        <tr>
                            <th>Terrain</th>
                            <th>410</th>
                            <td>0</td>
                            <td>0</td>
                            <td>16</td>
                        </tr>
                        <tr>
                            <th>Oak</th>
                            <td>0</td>
                            <th>260</th>
                            <td>10</td>
                            <td>0</td>
                        </tr>
                        <tr>
                            <th>Shrub</th>
                            <td>0</td>
                            <td>0</td>
                            <th>137</th>
                            <td>16</td>
                        </tr>
                        <tr>
                            <th>Grass</th>
                            <td>23</td>
                            <td>0</td>
                            <td>11</td>
                            <th>192</th>
                        </tr>
                    </table>
                </div>
            </div>

            <!-- Columns: Classification -->
            <div class="margin padding-xl">
                <div class="row">
                    <div class="column">
                        <img class="full-width" src="../medias/projects/colorply/colorply_result.gif" />
                    </div>
                    <div class="column">
                        <img class="full-width" src="../medias/projects/colorply/colorply_classification.gif" />
                    </div>
                </div>
                <!-- Legends -->
                <table>
                    <tr>
                        <th>Color</th>
                        <th>Terrain</th>
                        <th>Oak</th>
                        <th>Shrub</th>
                        <th>Grass</th>
                    </tr>
                    <tr>
                        <th>Vegetation</th>
                        <td>WHITE</td>
                        <td>GREEN</td>
                        <td>BLUE</td>
                        <td>RED</td>
                    </tr>
                </table>
            </div>
            <!-- End Columns -->
            <!-- End Classification Section -->

            <!-- Banner -->
            <div id="outcomes" class="banner-end bg-red mask-color">
                <div class="text margin">
                    <h2 class="white margin-title">Outcomes</h2>
                    <p class="white margin-xl">
                        This project have been the occassion to work on photogrammetry for remote sensing purposes. As my interest in imagery is growing, I particularly appreciate working on it, and I
                        will keep updating the repository on github.
                    </p>
                </div>
            </div>
            <!-- End Banner -->
        </div>
        <!-- End Project Work section -->

        <!-- Overlay effects -->
        <script src="../static/js/scroll.js"></script>
        <!-- Reveal on scroll -->
        <script src="../static/js/reveal.js"></script>
        <!-- Navbar Hamburger -->
        <script src="../static/js/navbar.js"></script>
        <!-- Charts -->
        <script src="../static/js/colorply/charts.js"></script>
    </body>
</html>
